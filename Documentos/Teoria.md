<style>
        body {
            background-color: white;
            color: black;
            transition: background-color 0.5s, color 0.5s; /* Esto a√±ade una suave transici√≥n al cambiar los colores */
        }

        body.dark-mode {
            background-color: #1E2021;
            color: white;
            
        }
      body.dark-mode strong ,body.dark-mode h1, body.dark-mode h2, body.dark-mode h3, body.dark-mode h4, body.dark-mode h5, body.dark-mode h6{
            color: white;
        }
      body.dark-mode blockquote{
         filter: invert(1);
      }
        button{
         font-family: 'Roboto', sans-serif;
         font-size: 14px;
         font-weight: bolt;
         background: #1e90FF;
         width: 100px;
         padding: 7px;
         text-align: center;
         text-decoration: none;
         text-transform: uppercase;
         color: #fff;
         border-radius: 6px;
         cursor: pointer;
         box-shadow: 0 0 20px rgba(0, 0, 0, 0.5);
        }
    </style>
<script>
    function toggleDarkMode() {
        const body = document.body;
        if (body.classList.contains('dark-mode')) {
            body.classList.remove('dark-mode');
        } else {
            body.classList.add('dark-mode');
        }
    }
</script>

<div align='center'><h2> ü§ñ Deep Learning</h2></div>


<div align='center'>
<button onclick="toggleDarkMode()">Oscuro</button>
<a href='https://github.com/Fabian-Martinez-Rincon/Deep-Learning/tree/main'><button style='background: #ecec00'> ‚≠ê </button></a>
<a href='https://github.com/Fabian-Martinez-Rincon'><button style='background: #000000'>Github</button></a>

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5
' height="10" width="100%"></div>


- [Practica](/index.html)
- [Polars (alternativa a pandas)](https://www.youtube.com/watch?v=CtkMzCIXOWk)

## 01a_Introduccion

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

"Discreta" y "continua" son t√©rminos que se refieren a dos tipos diferentes de variables o conjuntos de datos. Vamos a definir y diferenciar cada uno.

#### Discreta

Una variable discreta es aquella que puede tomar un n√∫mero finito o un conjunto contable infinito de valores. Estos valores suelen ser resultados espec√≠ficos y separados, y no pueden ser subdivididos de manera significativa.

<table>
<tr><td>Ejemplos</td><td>Caracteristicas</td></tr>
<tr><td>

- El n√∫mero de estudiantes en una clase (puedes tener 20, 21, 22 estudiantes, pero no 20.5 estudiantes).
- El n√∫mero de lanzamientos de una moneda hasta obtener cara.
- La cantidad de libros en una estanter√≠a.
</td><td>

- Los valores que puede tomar son contables.
- No tiene sentido hablar de valores "entre" dos valores discretos adyacentes. Por ejemplo, entre 3 y 4 estudiantes, no hay ning√∫n valor v√°lido.
</td></tr>
</table>

#### Continua

Una variable continua es aquella que puede tomar cualquier valor dentro de un rango espec√≠fico. Estos valores pueden ser subdivididos infinitamente, y hay un n√∫mero infinito de valores posibles entre dos valores cualesquiera.


<table>
<tr><td>Ejemplos</td><td>Caracteristicas</td></tr>
<tr><td>

- La altura de una persona (puede ser 1.75 metros, 1.751 metros, 1.7512 metros, etc.).
- El tiempo que tarda un corredor en completar una marat√≥n.
- La temperatura de una habitaci√≥n.
</td><td>

- Los valores que puede tomar son incontables dentro de un rango.
- Siempre hay valores intermedios entre dos valores cualesquiera, sin importar cu√°n cercanos est√©n.
</td></tr>
</table>




<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

### Aprendizaje Supervisado

> Video de Regresi√≥n Lineal Dot Csv [Link](https://www.youtube.com/watch?v=k964_uNn3l0)

El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde se proporciona un conjunto de datos de entrada junto con las respuestas correctas (etiquetas) con el objetivo de entrenar un modelo que pueda hacer predicciones precisas sobre datos no vistos previamente.

#### Clasificaci√≥n


En problemas de clasificaci√≥n, el objetivo es predecir una etiqueta categ√≥rica o discreta para una entrada dada. Es decir, se clasifica la entrada en una de varias categor√≠as o clases.

<table><td>


- Determinar si un correo electr√≥nico es spam o no spam.
- Clasificar una imagen en categor√≠as como "gato", "perro" o "p√°jaro".
- Diagnosticar si un paciente tiene una enfermedad espec√≠fica o no.

</td><td>



<img src='https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/37c37b79-0381-4afb-b898-d04a698eec25' width='400px'>
</td></table>



#### Regresi√≥n

En problemas de regresi√≥n, el objetivo es predecir un valor continuo para una entrada dada. A diferencia de la clasificaci√≥n, donde se predice una etiqueta de clase, en la regresi√≥n se predice una cantidad num√©rica. (Rojo: Los datos reales, Azul: Datos previstos)

<table><td>

- Predecir el precio de una casa basado en sus caracter√≠sticas (tama√±o, ubicaci√≥n, n√∫mero de habitaciones, etc.).
- Estimar la temperatura de una ciudad en un d√≠a futuro.
- Prever las ventas de un producto en los pr√≥ximos meses.



</td><td>



<img src='https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/497e06a4-6da7-4950-becd-2483bb48f0aa' width='800px'>
</td></table>


#### Diferencias

<table>
<tr><td>Clasificaci√≥n</td><td>Regresi√≥n</td></tr>
<tr><td>

- Las respuestas (etiquetas) son discretas y pertenecen a un conjunto finito.
- No hay orden inherente entre las clases. Por ejemplo, en una clasificaci√≥n de animales, "gato" no es mayor ni menor que "perro".
</td><td>

- Las respuestas son valores num√©ricos continuos.
- Hay un orden inherente en las respuestas. Por ejemplo, un precio de casa de \$300,000 es mayor que uno de \$200,000.
</td></tr>
</table>

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

## Red Neuronal Artificial


> Preguntar sobre los pesos y la funcion de activaci√≥n
> Video de [Dot csv Parte 1](https://www.youtube.com/watch?v=MRIv2IwFTPg)
> Video de [Dot csv Parte 2](https://www.youtube.com/watch?v=uwbHOpp9xkc)
> Video de [Dot csv Parte 2.5](https://www.youtube.com/watch?v=FVozZVUNOOA)
> Video de [Dot csv Parte 3](https://www.youtube.com/watch?v=eNIqz_noix8)
> Video de [Dot csv Parte 3.5](https://www.youtube.com/watch?v=M5QHwkkHgAA&t=1s)


<table><td>

Una Red Neuronal Artificial (RNA) es un modelo de aprendizaje autom√°tico inspirado en la estructura y funcionamiento de las redes neuronales biol√≥gicas presentes en el cerebro humano. Estas redes est√°n compuestas por unidades llamadas "neuronas" que est√°n interconectadas y trabajan juntas para producir una salida basada en una serie de entradas.
</td><td>

<img src='https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/ff926135-69ca-4696-8175-75d1ce5980dd' width='3000'>
</td></table>

#### Componentes b√°sicos de una RNA

1. **Neuronas**: Son las unidades b√°sicas de procesamiento en una red neuronal. Cada neurona recibe una serie de entradas, las procesa y produce una salida.

2. **Pesos**: Son valores asociados a las conexiones entre neuronas. Determinan la importancia relativa de cada entrada a una neurona.

3. **Funci√≥n de Activaci√≥n**: Es una funci√≥n matem√°tica aplicada a la salida de una neurona. Introduce no linealidades en la red, permitiendo que la RNA aprenda relaciones m√°s complejas.

4. **Bias**: Es un t√©rmino adicional que se a√±ade antes de la funci√≥n de activaci√≥n para permitir que la neurona se ajuste mejor a los datos.

#### Estructura de una RNA

1. **Capa de Entrada**: Es la primera capa de la red y recibe los datos de entrada.

2. **Capas Ocultas**: Son las capas intermedias entre la entrada y la salida. Pueden ser una o varias, dependiendo de la profundidad de la red.

3. **Capa de Salida**: Es la √∫ltima capa de la red y produce la salida final, que puede ser una clasificaci√≥n, una regresi√≥n, entre otros.

#### Funcionamiento

1. **Feedforward (Propagaci√≥n hacia adelante)**: Los datos de entrada se pasan a trav√©s de la red, desde la capa de entrada hasta la capa de salida. En cada neurona, se calcula una suma ponderada de las entradas, se a√±ade el bias y se aplica la funci√≥n de activaci√≥n para producir la salida.

2. **Backpropagation (Retropropagaci√≥n)**: Es el algoritmo utilizado para entrenar una RNA. Consiste en calcular el error de la red (diferencia entre la salida predicha y la real) y propagar este error hacia atr√°s para ajustar los pesos y biases de la red.

3. **Aprendizaje**: Durante el entrenamiento, la RNA ajusta repetidamente sus pesos y biases utilizando un algoritmo de optimizaci√≥n (como el descenso del gradiente) para minimizar el error en las predicciones.



<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

## Redes Neuronales Convolucionales

> Video de Dot Csv: [Redes Neuronales Convolucionales](https://www.youtube.com/watch?v=V8j1oENVz00)

Una Red Neuronal Convolucional es un tipo especializado de red neuronal dise√±ada espec√≠ficamente para procesar datos con una estructura similar a una cuadr√≠cula, como im√°genes.

![image](https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/3eafca50-417f-4487-a85a-67a125350401)

1. **Capas Convolucionales**: Estas capas aplican un conjunto de filtros a la entrada. Cada filtro detecta caracter√≠sticas espec√≠ficas, como bordes, texturas o colores. La operaci√≥n de convoluci√≥n escanea la imagen con un peque√±o "ventana" y aplica una transformaci√≥n para producir un mapa de caracter√≠sticas.

2. **Funci√≥n de Activaci√≥n**: Despu√©s de cada convoluci√≥n, se aplica una funci√≥n de activaci√≥n, como la funci√≥n ReLU (Rectified Linear Unit), para introducir no linealidades en el modelo. [Mas informaci√≥n](/Documentos/FuncionesDeActivacion.html)

3. **Pooling o Submuestreo**: Estas capas reducen la dimensionalidad espacial (ancho y alto) de los datos, conservando las caracter√≠sticas m√°s importantes. Una t√©cnica com√∫n es el "max pooling", que selecciona el valor m√°ximo de un grupo de valores.

4. **Capas Totalmente Conectadas (FC, Fully Connected)**: Despu√©s de varias capas convolucionales y de pooling, las CNNs suelen tener una o m√°s capas totalmente conectadas que procesan la informaci√≥n y producen la salida final, como una clasificaci√≥n.

5. **Clasificaci√≥n**: La √∫ltima capa de una CNN generalmente produce una distribuci√≥n de probabilidad sobre las clases objetivo, utilizando una funci√≥n como la funci√≥n softmax.

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

## Redes Neuronales que generan datos

Las redes neuronales que generan datos son modelos avanzados que se entrenan para producir datos que se asemejen a algunos datos de entrada reales. 
1. **Redes Generativas Adversarias (GANs)**:
   - **Funcionamiento**: Las GANs consisten en dos redes neuronales, el generador y el discriminador, que se entrenan simult√°neamente a trav√©s de un juego adversario. El generador intenta producir datos que enga√±en al discriminador, mientras que el discriminador intenta distinguir entre datos reales y datos generados.
   - **Aplicaciones**: Creaci√≥n de im√°genes realistas, mejora de la resoluci√≥n de im√°genes, generaci√≥n de arte, creaci√≥n de modelos 3D, entre otros.

2. **Modelos Autoregresivos**:
   - **Funcionamiento**: Estos modelos generan datos de manera secuencial, prediciendo el siguiente elemento bas√°ndose en los elementos anteriores.
   - **Ejemplos**: PixelRNN, PixelCNN, y modelos de lenguaje como GPT (Generative Pre-trained Transformer).
   - **Aplicaciones**: Generaci√≥n de texto, im√°genes y m√∫sica.

3. **Redes Neuronales Variacionales (VAEs)**:
   - **Funcionamiento**: Las VAEs son una clase de modelos generativos que aprenden a representar y generar datos en un espacio latente de manera probabil√≠stica.
   - **Aplicaciones**: Generaci√≥n de im√°genes, interpolaci√≥n de datos, y aprendizaje de representaciones latentes.

4. **Modelos de Transformaci√≥n Espacial**:
   - **Funcionamiento**: Estos modelos aprenden transformaciones espaciales para modificar datos de entrada de manera espec√≠fica.
   - **Aplicaciones**: Alineaci√≥n y transformaci√≥n de im√°genes y datos en general.

5. **Redes Generativas de Flujos Normales**:
   - **Funcionamiento**: Estos modelos utilizan transformaciones invertibles y se entrenan maximizando la verosimilitud de los datos.
   - **Aplicaciones**: Generaci√≥n de im√°genes y otros tipos de datos.

6. **Modelos basados en memoria**:
   - **Funcionamiento**: Estos modelos almacenan ejemplos de datos y generan nuevos datos bas√°ndose en la similitud con los datos almacenados.
   - **Aplicaciones**: Generaci√≥n basada en ejemplos anteriores, como en la generaci√≥n de m√∫sica.


<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

## Tipos de Variables


Las variables se pueden clasificar en cuantitativas y cualitativas seg√∫n la naturaleza de la informaci√≥n que representan. A continuaci√≥n, se describen los tipos de cada una:

#### Variables Cuantitativas:

Son aquellas que representan una cantidad y pueden ser medidas num√©ricamente. Se dividen en:

1. **Discretas**: Estas variables pueden tomar valores espec√≠ficos y contables, generalmente enteros. No pueden tener valores fraccionarios entre dos valores consecutivos.
   - **Ejemplo**: N√∫mero de hijos en una familia, cantidad de estudiantes en un aula.

2. **Continuas**: Estas variables pueden tomar cualquier valor dentro de un rango espec√≠fico. Pueden tener valores fraccionarios.
   - **Ejemplo**: Altura de una persona, peso, temperatura.

#### Variables Cualitativas:

Son aquellas que describen caracter√≠sticas o cualidades que no pueden ser medidas con n√∫meros. Se dividen en:

1. **Nominales**: Estas variables representan categor√≠as o etiquetas sin un orden inherente.
   - **Ejemplo**: Color de ojos (azul, verde, marr√≥n), g√©nero (masculino, femenino), tipo de veh√≠culo (sed√°n, camioneta, convertible).

2. **Ordinales**: Estas variables representan categor√≠as con un orden o jerarqu√≠a espec√≠fico, pero la distancia entre las categor√≠as no es uniforme ni tiene un significado espec√≠fico.
   - **Ejemplo**: Niveles de educaci√≥n (primaria, secundaria, universitaria), grados de satisfacci√≥n (insatisfecho, neutral, satisfecho).

>Es importante mencionar que, en algunas situaciones, las variables cualitativas ordinales pueden ser codificadas num√©ricamente para facilitar su an√°lisis (por ejemplo, asignando 1 a "insatisfecho", 2 a "neutral" y 3 a "satisfecho"). Sin embargo, esto no convierte la variable en cuantitativa, ya que los n√∫meros simplemente representan categor√≠as con un orden espec√≠fico y no tienen un significado cuantitativo real.

La correcta identificaci√≥n y clasificaci√≥n de las variables es esencial en estad√≠stica y investigaci√≥n, ya que determina qu√© tipos de an√°lisis son apropiados para los datos recopilados.

#### Ejemplo Practico

```markdown
| Nro. | Age | Sex | BP     | Colesterol | Na       | K       | Drug  |
|------|-----|-----|--------|------------|----------|---------|-------|
| 1    | 23  | F   | HIGH   | HIGH       | 0.792535 | 0.031258| drugY |
| 2    | 47  | M   | LOW    | HIGH       | 0.739309 | 0.056468| drugC |
| 3    | 47  | M   | LOW    | HIGH       | 0.697269 | 0.068944| drugC |
| 4    | 28  | F   | NORMAL | HIGH       | 0.563682 | 0.072289| drugX |
| 5    | 61  | F   | LOW    | HIGH       | 0.559294 | 0.030998| drugY |
| ...  | ... | ... | ...    | ...        | ...      | ...     | ...   |
| 197  | 16  | M   | LOW    | HIGH       | 0.743021 | 0.061886| drugC |
| 198  | 52  | M   | NORMAL | HIGH       | 0.549945 | 0.055581| drugX |
| 199  | 23  | M   | NORMAL | NORMAL     | 0.78452  | 0.055959| drugX |
| 200  | 40  | F   | LOW    | NORMAL     | 0.683503 | 0.060226| drugX |
```

Podemos clasificar el tipo de cada atributo de la siguiente manera:

1. **Nro.**: Cuantitativo Discreto - Representa un n√∫mero de identificaci√≥n o secuencia.
2. **Age**: Cuantitativo Discreto - Representa la edad en a√±os completos.
3. **Sex**: Cualitativo Nominal - Indica el g√©nero (F para femenino, M para masculino).
4. **BP (Presi√≥n arterial)**: Cualitativo Ordinal - Indica el nivel de presi√≥n arterial (HIGH, LOW, NORMAL).
5. **Colesterol**: Cualitativo Ordinal - Indica el nivel de colesterol (HIGH, NORMAL).
6. **Na (Sodio)**: Cuantitativo Continuo - Representa una medida num√©rica del nivel de sodio.
7. **K (Potasio)**: Cuantitativo Continuo - Representa una medida num√©rica del nivel de potasio.
8. **Drug**: Cualitativo Nominal - Indica el tipo de medicamento (drugY, drugC, drugX, etc.).

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

## Descripciones Estad√≠sticas B√°sicas
Las descripciones estad√≠sticas b√°sicas son herramientas fundamentales para entender y resumir un conjunto de datos. Estas medidas son esenciales para tomar decisiones informadas sobre c√≥mo procesar y analizar los datos

#### Medidas de Tendencia Central
Estas medidas indican d√≥nde se encuentra el "centro" de un conjunto de datos.

1. **Media (Promedio)**
   - Es la suma de todos los valores en un conjunto de datos dividida por el n√∫mero de valores en ese conjunto.
   - F√≥rmula:
     \[
     \bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}
     \]

2. **Mediana**
   - Es el valor que divide un conjunto de datos en dos mitades, de modo que la mitad de los valores son menores o iguales a la mediana y la otra mitad son mayores o iguales.

3. **Moda**
   - Es el valor o valores que aparecen con mayor frecuencia en un conjunto de datos.

4. **Rango Medio**
   - Es el promedio del valor m√°s bajo y el valor m√°s alto en un conjunto de datos.
   - F√≥rmula:
     \[
     \text{Rango Medio} = \frac{\text{Valor M√≠nimo} + \text{Valor M√°ximo}}{2}
     \]

#### Medidas de Dispersi√≥n
Estas medidas indican cu√°nto var√≠an los datos alrededor de la tendencia central.

1. **Varianza**
   - Es el promedio de las diferencias al cuadrado entre cada valor y la media.
   - F√≥rmula para la varianza de una poblaci√≥n (denotada como \(\sigma^2\)):
     \[
     \sigma^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n}
     \]
   - F√≥rmula para la varianza de una muestra (denotada como \(s^2\)):
     \[
     s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}
     \]

2. **Desviaci√≥n Est√°ndar**
   - Es la ra√≠z cuadrada de la varianza.
   - F√≥rmulas:
     \[
     \sigma = \sqrt{\sigma^2}
     \]
     \[
     s = \sqrt{s^2}
     \]

3. **Rango**
   - Es la diferencia entre el valor m√°ximo y el valor m√≠nimo en un conjunto de datos.

4. **Cuartiles**
    Dividen un conjunto de datos en cuatro partes iguales. El primer cuartil (Q1) es el valor que separa el 25% inferior de los datos, el segundo cuartil (Q2) es la mediana y el tercer cuartil (Q3) es el valor que separa el 75% inferior de los datos.

5. **Rango Intercuartil (IQR)**
   - Es la diferencia entre el tercer y el primer cuartil. Ayuda a medir la dispersi√≥n de los valores en el medio 50% de un conjunto de datos.
   - F√≥rmula:
     \[
     IQR = Q3 - Q1
     \]

Estas medidas son esenciales para comprender la distribuci√≥n, la tendencia central y la variabilidad de un conjunto de datos.

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

## Graficos (Un choclo)

#### 1. Diagrama de Barras



<table><td>

- **Descripci√≥n**: Representa datos categ√≥ricos con barras rectangulares de longitudes proporcionales a los valores que representan.
- **Uso**: Se utiliza para comparar diferentes categor√≠as de datos.
- **Ejemplo**: Puedes usar un diagrama de barras para mostrar las ventas de diferentes productos en un mes.
</td><td>


<img src='https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/8a8e7142-3fe2-4a57-9345-68e6cb5f133c' width='1000px'>
</td></table>



#### 2. Diagrama de Torta (o Gr√°fico Circular)

<table><td>

- **Descripci√≥n**: Representa datos en proporciones a un todo. Cada "rebanada" de la torta representa una categor√≠a.
- **Uso**: Se utiliza para mostrar proporciones o porcentajes.
- **Ejemplo**: Puedes usar un diagrama de torta para mostrar la distribuci√≥n del presupuesto de una empresa entre diferentes departamentos.
</td><td><img src='https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/4a8cf0d5-f2ec-459c-ba5c-9c57e2580193' width='1000px'></td></table>



#### 3. Histograma
- **Descripci√≥n**: Es similar a un diagrama de barras, pero se utiliza para representar la distribuci√≥n de un conjunto de datos continuos o intervalos.
- **Uso**: Se utiliza para visualizar la distribuci√≥n de frecuencias de un conjunto de datos.
- **Ejemplo**: Puedes usar un histograma para mostrar la distribuci√≥n de edades de los empleados en una empresa.

![image](https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/ac455d0b-0f26-499a-8a47-49f8076afbb0)


#### 4. Diagrama de Caja (o Boxplot)
El diagrama de cajas, tambi√©n conocido como "box plot" o "whisker plot", es una herramienta gr√°fica que proporciona una representaci√≥n visual de la distribuci√≥n y dispersi√≥n de un conjunto de datos. Es especialmente √∫til para identificar la mediana, los cuartiles y posibles valores at√≠picos en los datos.


![image](https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/96759b0b-5b8a-483c-b9f1-f52dbbc928d2)

Se consideran valores at√≠picos leves a los que se encuentran a 1.5\*RIC m√°s all√° de los l√≠mites de la caja y at√≠picos extremos a los que est√°n m√°s all√° de 3\*RIC.

A continuaci√≥n, te explico sus componentes y c√≥mo interpretarlos:

1. **Caja Central:**
   - La caja en s√≠ representa el rango intercuartil (IQR), que es la distancia entre el primer cuartil (Q1) y el tercer cuartil (Q3). El IQR contiene la mitad central de los datos.
   - La l√≠nea dentro de la caja representa la mediana (Q2), que es el valor que divide el conjunto de datos en dos mitades. La mediana indica el valor central de los datos.

2. **Bigotes (Whiskers):**
   - Los "bigotes" son las l√≠neas que se extienden desde la caja hacia los valores m√°ximo y m√≠nimo dentro de un rango determinado.
   - El bigote superior se extiende desde Q3 hasta el valor m√°ximo dentro de 1.5*IQR de Q3.
   - El bigote inferior se extiende desde Q1 hasta el valor m√≠nimo dentro de 1.5*IQR de Q1.
   - Los valores fuera de este rango se consideran valores at√≠picos y a menudo se representan con puntos o asteriscos fuera de los bigotes.

3. **Valores At√≠picos:**
   - Son los puntos que caen fuera del rango de los bigotes. Estos puntos representan valores que son inusualmente altos o bajos en comparaci√≥n con el resto del conjunto de datos.

4. **Rango:**
   - Es la distancia entre el valor m√≠nimo y el valor m√°ximo del conjunto de datos, incluidos los valores at√≠picos.

**Interpretaci√≥n:**
- **Distribuci√≥n de los datos:** Si la mediana est√° cerca del centro de la caja, los datos est√°n aproximadamente sim√©tricamente distribuidos. Si est√° m√°s cerca de Q1 o Q3, los datos est√°n sesgados.
- **Dispersi√≥n:** El ancho de la caja (IQR) indica la variabilidad de los datos. Una caja m√°s ancha sugiere una mayor dispersi√≥n.
- **Valores At√≠picos:** Los puntos fuera de los bigotes indican valores at√≠picos que pueden requerir una investigaci√≥n adicional.
- **Comparaci√≥n:** Los diagramas de cajas son √∫tiles para comparar la distribuci√≥n de varios conjuntos de datos en paralelo.

El diagrama de cajas es una herramienta poderosa para visualizar la distribuci√≥n de un conjunto de datos, identificar su variabilidad y detectar valores at√≠picos. Es ampliamente utilizado en estad√≠sticas y an√°lisis de datos para obtener una r√°pida visi√≥n general de la distribuci√≥n de los datos.

![image](https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/5c6b7a62-8456-4b03-87af-a7bce9e6021d)

#### 5. Diagrama de Dispersi√≥n (o Scatter Plot)
- **Descripci√≥n**: Muestra puntos en un plano cartesiano, donde cada punto representa una observaci√≥n con dos variables.
- **Uso**: Se utiliza para visualizar la relaci√≥n entre dos variables cuantitativas.
- **Ejemplo**: Puedes usar un diagrama de dispersi√≥n para mostrar la relaci√≥n entre las horas de estudio y las calificaciones obtenidas por los estudiantes.

![image](https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/40cc78bd-fc43-4f5e-a14e-f50930955338)

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

### Relaci√≥n entre atributos num√©ricos

<table><td>

Al momento de construir un modelo resulta de inter√©s saber si dos atributos num√©ricos se encuentran linealmente relacionados o no. Para ello se usa el coeficiente de correlaci√≥n lineal.
</td><td>

<img src='https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/2f9ee564-ae47-494b-9953-9c83dd33887c' width='1300px'>
</td></table>

> Diagrama de dispersi√≥n entre la longitud y el ancho del p√©talo de una flor

### Coeficiente de correlaci√≥n lineal

- Si 0.5‚â§  abs(Corr(A,B)) < 0.8 se dice que A y B tienen una correlaci√≥n lineal d√©bil.
- Si abs(Corr(A,B)) ‚â• 0.8 se dice que A y B tienen una correlaci√≥n lineal fuerte
- Si abs(Corr(A,B))<0.5 se dice que A y B no est√°n correlacionados linealmente. Esto NO implica que son independientes, s√≥lo que entre ambos no hay una correlaci√≥n lineal

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

## 01b_Analisis de datos



<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7eebf649-e558-43e2-ad5f-9977dc5ff3e5' height="10" width="100%">

## 01c_Preprocesamiento de datos

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7c13752b-5a48-4a88-816d-242330e90478' height="10" width="100%">


# Conceptos Fuera de la teoria pero que sirven para reforsar el conocimiento


<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7c13752b-5a48-4a88-816d-242330e90478' height="10" width="100%">

# Derivadas Parciales

Las derivadas parciales son un concepto fundamental en c√°lculo multivariable y tienen aplicaciones en diversas √°reas, como la f√≠sica, la ingenier√≠a y, por supuesto, en el aprendizaje autom√°tico y las redes neuronales.

### Definici√≥n:

Una derivada parcial de una funci√≥n de varias variables es la derivada de esa funci√≥n con respecto a una de sus variables, manteniendo las otras variables constantes.

### Notaci√≥n:

Si \( f \) es una funci√≥n de las variables \( x \) y \( y \), entonces la derivada parcial de \( f \) con respecto a \( x \) se denota de las siguientes maneras:
\[ \frac{\partial f}{\partial x} \quad \text{o} \quad f_x \]

De manera similar, la derivada parcial de \( f \) con respecto a \( y \) se denota como:
\[ \frac{\partial f}{\partial y} \quad \text{o} \quad f_y \]

### Ejemplo:

Considera la funci√≥n \( f(x, y) = x^2y + xy^2 \).

1. La derivada parcial de \( f \) con respecto a \( x \) es:
\[ \frac{\partial f}{\partial x} = 2xy + y^2 \]

2. La derivada parcial de \( f \) con respecto a \( y \) es:
\[ \frac{\partial f}{\partial y} = x^2 + 2xy \]

### Aplicaciones en Aprendizaje Autom√°tico:

Las derivadas parciales son esenciales en el aprendizaje autom√°tico, especialmente en el entrenamiento de redes neuronales. Cuando se ajusta un modelo, se busca minimizar una funci√≥n de p√©rdida o costo. Para hacerlo, se utiliza el algoritmo de descenso del gradiente, que requiere calcular el gradiente de la funci√≥n de p√©rdida. El gradiente es simplemente un vector que contiene todas las derivadas parciales de la funci√≥n con respecto a cada uno de los par√°metros del modelo. Estas derivadas parciales indican c√≥mo cambiar cada par√°metro para reducir el error.

En resumen, las derivadas parciales proporcionan una manera de entender c√≥mo una funci√≥n cambia en una direcci√≥n espec√≠fica en un espacio multidimensional, y son herramientas cruciales en muchos campos de la ciencia y la ingenier√≠a.

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7c13752b-5a48-4a88-816d-242330e90478' height="10" width="100%">

# Suma Ponderada

Una suma ponderada es una combinaci√≥n lineal de ciertos valores donde cada valor tiene un peso o ponderaci√≥n asociado. En lugar de sumar los valores directamente, cada valor se multiplica por su peso correspondiente y luego se suman los resultados.

La f√≥rmula general para una suma ponderada de \( n \) valores es:

\[ \text{Suma ponderada} = w_1 \cdot x_1 + w_2 \cdot x_2 + \ldots + w_n \cdot x_n \]

Donde:
- \( x_1, x_2, \ldots, x_n \) son los valores.
- \( w_1, w_2, \ldots, w_n \) son los pesos o ponderaciones correspondientes a cada valor.

### Ejemplo:

Supongamos que estamos calculando el promedio final de un estudiante en un curso que tiene tres componentes: tareas, ex√°menes y un proyecto final. Cada componente tiene un peso diferente en la calificaci√≥n final:

- Tareas: 40%
- Ex√°menes: 50%
- Proyecto final: 10%

Si el estudiante obtuvo las siguientes calificaciones:
- Tareas: 85
- Ex√°menes: 90
- Proyecto final: 95

La calificaci√≥n final ponderada ser√≠a:

\[ \text{Calificaci√≥n final} = 0.40 \times 85 + 0.50 \times 90 + 0.10 \times 95 = 88 \]

Por lo tanto, la calificaci√≥n final del estudiante ser√≠a 88.

### Aplicaciones:

Las sumas ponderadas son comunes en muchos campos:

1. **Aprendizaje autom√°tico**: En una neurona artificial de una red neuronal, la entrada se combina mediante una suma ponderada antes de pasar por una funci√≥n de activaci√≥n.
2. **Econom√≠a**: Para calcular √≠ndices como el √çndice de Precios al Consumidor (IPC), se utiliza una suma ponderada de los precios de diferentes bienes y servicios.
3. **Educaci√≥n**: Como en el ejemplo anterior, para calcular calificaciones finales basadas en diferentes componentes con diferentes ponderaciones.
4. **Investigaci√≥n y an√°lisis de datos**: Al combinar diferentes indicadores o m√©tricas en un solo valor o √≠ndice.

En resumen, una suma ponderada es una herramienta matem√°tica √∫til que permite combinar diferentes valores teniendo en cuenta su relevancia o importancia relativa.

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7c13752b-5a48-4a88-816d-242330e90478' height="10" width="100%">

# Funcion de activaci√≥n

La funci√≥n de activaci√≥n en una red neuronal introduce no linealidades en el modelo, lo que le permite aprender y representar relaciones m√°s complejas en los datos. Veamos qu√© significa esto con m√°s detalle:

### Linealidad vs. No Linealidad:

1. **Funciones Lineales**: Una funci√≥n es lineal si cumple con dos propiedades: aditividad y homogeneidad. En t√©rminos simples, una funci√≥n lineal tiene la forma \(f(x) = mx + b\), donde \(m\) y \(b\) son constantes. No importa cu√°ntas capas lineales apiles una encima de la otra en una red neuronal, la combinaci√≥n seguir√° siendo una transformaci√≥n lineal. Esto limita la capacidad de la red para representar funciones m√°s complejas.

2. **Funciones No Lineales**: Estas funciones no cumplen con las propiedades de aditividad y homogeneidad. Introducen curvas y permiten que la funci√≥n tenga una forma m√°s compleja. Las funciones no lineales pueden representar una amplia variedad de relaciones y patrones en los datos.

### Importancia de las No Linealidades en Redes Neuronales:

- **Capacidad de Modelado**: Al introducir no linealidades, una red neuronal puede aprender y modelar relaciones complejas en los datos. Esto es esencial para tareas como la clasificaci√≥n de im√°genes, donde las relaciones entre los p√≠xeles pueden ser altamente no lineales.

- **Profundidad y Aprendizaje Jer√°rquico**: Las no linealidades permiten que las redes neuronales profundas aprendan caracter√≠sticas a diferentes niveles de abstracci√≥n. Por ejemplo, en una CNN que procesa im√°genes, las primeras capas pueden aprender a detectar bordes, las capas intermedias pueden aprender a detectar formas, y las capas m√°s profundas pueden aprender a detectar objetos completos.

### Ejemplos de Funciones de Activaci√≥n No Lineales:

1. **ReLU (Rectified Linear Unit)**: \(f(x) = max(0, x)\). Es una de las funciones de activaci√≥n m√°s populares en redes neuronales profundas debido a su simplicidad y eficacia.

2. **Sigmoid**: \(f(x) = \frac{1}{1 + e^{-x}}\). Toma valores entre 0 y 1.

3. **Tanh (Tangente Hiperb√≥lica)**: \(f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\). Toma valores entre -1 y 1.

4. **Leaky ReLU**: Una variante de ReLU que permite peque√±os valores negativos cuando \(x < 0\).

En resumen, las no linealidades son esenciales para la capacidad de las redes neuronales para aprender y representar relaciones complejas en los datos. Sin ellas, la red ser√≠a simplemente una combinaci√≥n lineal de sus entradas, lo que limitar√≠a severamente su utilidad.

![image](https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/f8906f0d-e728-4bd4-a5ab-acfaa18dc0b5)

## Explicaci√≥n mas detallada

1. **ReLU (Rectified Linear Unit)**:
   - **F√≥rmula**: \(f(x) = max(0, x)\)
   - **Descripci√≥n**: La funci√≥n ReLU es bastante simple. Si el valor de entrada \(x\) es positivo, devuelve \(x\). Si \(x\) es negativo o cero, devuelve 0.
   - **Gr√°ficamente**: Imagina una l√≠nea que se eleva a 45 grados para valores positivos de \(x\) y se aplana completamente para valores negativos.
   - **Uso**: Es ampliamente utilizado en redes neuronales profundas porque es computacionalmente eficiente y ayuda a mitigar el problema del desvanecimiento del gradiente en cierta medida.

2. **Sigmoid**:
   - **F√≥rmula**: \(f(x) = \frac{1}{1 + e^{-x}}\)
   - **Descripci√≥n**: La funci√≥n sigmoid toma cualquier valor de entrada y lo "aprieta" en un rango entre 0 y 1. Es una funci√≥n en forma de "S".
   - **Gr√°ficamente**: Imagina una curva en forma de "S" que se aplana a medida que se acerca a 0 y 1.
   - **Uso**: Fue ampliamente utilizado en las primeras redes neuronales, especialmente como funci√≥n de activaci√≥n en la capa de salida para clasificaci√≥n binaria. Sin embargo, tiene problemas de desvanecimiento del gradiente en redes profundas, por lo que su uso ha disminuido en favor de ReLU y variantes.

3. **Tanh (Tangente Hiperb√≥lica)**:
   - **F√≥rmula**: \(f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)
   - **Descripci√≥n**: Similar a la funci√≥n sigmoid, pero en lugar de comprimir los valores en el rango [0,1], los comprime en el rango [-1,1].
   - **Gr√°ficamente**: Imagina una curva en forma de "S" similar a la sigmoid, pero que se extiende desde -1 hasta 1 en lugar de 0 hasta 1.
   - **Uso**: Es preferible a la funci√≥n sigmoid en muchos casos porque los valores centrados alrededor de 0 facilitan el aprendizaje.

4. **Leaky ReLU**:
   - **F√≥rmula**: \(f(x) = x\) si \(x > 0\), \(f(x) = \alpha x\) si \(x \leq 0\), donde \(\alpha\) es un peque√±o valor positivo (por ejemplo, 0.01).
   - **Descripci√≥n**: Es una variante de ReLU que permite valores negativos peque√±os cuando la entrada es menor que cero. Esto ayuda a evitar que las neuronas "mueran" y no se activen en absoluto.
   - **Gr√°ficamente**: Similar a ReLU, pero en lugar de ser completamente plana para valores negativos, tiene una pendiente leve (determinada por \(\alpha\)).
   - **Uso**: Se utiliza como una soluci√≥n al problema de las "neuronas muertas" en ReLU, donde algunas neuronas dejan de contribuir y se vuelven inactivas.

Estas funciones de activaci√≥n introducen no linealidades en la red, lo que permite que la red aprenda relaciones m√°s complejas en los datos. La elecci√≥n de la funci√≥n de activaci√≥n puede depender de la naturaleza del problema y la arquitectura de la red.

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7c13752b-5a48-4a88-816d-242330e90478' height="10" width="100%">

# Descenso del Gradiente

> Video de Dot csv sobre [Descenso gradiente](https://www.youtube.com/watch?v=A6FiCDoz8_4)



El descenso del gradiente es un algoritmo de optimizaci√≥n utilizado para minimizar una funci√≥n objetivo, que suele ser una funci√≥n de p√©rdida o costo en el contexto del aprendizaje autom√°tico. Es especialmente √∫til para entrenar modelos con muchos par√°metros, como las redes neuronales.

La idea detr√°s del descenso del gradiente es ajustar iterativamente los par√°metros del modelo en la direcci√≥n que reduce el error, es decir, en la direcci√≥n opuesta al gradiente de la funci√≥n de p√©rdida con respecto a esos par√°metros.

### Funcionamiento b√°sico del descenso del gradiente:

1. **Inicializaci√≥n**: Selecciona un punto inicial (un conjunto inicial de par√°metros) y un valor para la tasa de aprendizaje.

2. **C√°lculo del Gradiente**: Calcula el gradiente de la funci√≥n de p√©rdida con respecto a los par√°metros en el punto actual. El gradiente es un vector que apunta en la direcci√≥n de m√°ximo crecimiento de la funci√≥n, y su magnitud indica cu√°n r√°pido crece la funci√≥n en esa direcci√≥n.

3. **Actualizaci√≥n de Par√°metros**: Ajusta los par√°metros en la direcci√≥n opuesta al gradiente. La magnitud del ajuste est√° determinada por la tasa de aprendizaje.

4. **Iteraci√≥n**: Repite los pasos 2 y 3 hasta que el gradiente sea muy peque√±o (lo que indica que est√°s cerca de un m√≠nimo) o hasta que se alcance un n√∫mero m√°ximo de iteraciones.

### Variantes y consideraciones:

- **Tasa de Aprendizaje**: Es un hiperpar√°metro que determina el tama√±o de los pasos que se toman en cada iteraci√≥n. Una tasa de aprendizaje demasiado alta puede hacer que el algoritmo oscile y no converja, mientras que una tasa de aprendizaje demasiado baja puede hacer que la convergencia sea muy lenta.

- **Descenso del Gradiente Estoc√°stico (SGD)**: En lugar de usar todo el conjunto de datos para calcular el gradiente en cada paso (lo que puede ser costoso en t√©rminos de tiempo y memoria), el SGD utiliza un solo ejemplo o un peque√±o lote (batch) en cada iteraci√≥n.

- **M√©todos con Momento**: Estos m√©todos, como el descenso del gradiente con momento o Adam, utilizan informaci√≥n de gradientes anteriores para acelerar la convergencia y superar m√≠nimos locales.

- **M√≠nimos Locales**: Una de las preocupaciones con el descenso del gradiente es que puede quedar atrapado en m√≠nimos locales, especialmente en funciones no convexas. Sin embargo, en la pr√°ctica, especialmente en el contexto de redes neuronales profundas, esto suele ser menos problem√°tico de lo que se podr√≠a pensar.

El descenso del gradiente es fundamental en el aprendizaje autom√°tico y el aprendizaje profundo, y entender su funcionamiento y sus variantes es esencial para entrenar modelos eficientemente.

![1_yd9YMy2pkzCY-9jrsWFesA](https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/c8e13577-0171-41af-8c93-0d34305992c2)

<img src= 'https://github.com/Fabian-Martinez-Rincon/Fabian-Martinez-Rincon/assets/55964635/7c13752b-5a48-4a88-816d-242330e90478' height="10" width="100%">